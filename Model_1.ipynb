{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tux6uLXhOAaF"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErIw7bNuOAaK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from utils import download_data, load_data, data_augmentation, get_labels\n",
    "from visualization import show_sample, plot_loss, show_prediction\n",
    "from models import init_model_1, init_model_2, training, save_model, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsmhZpbbRMET"
   },
   "outputs": [],
   "source": [
    "data_path = './data' #path of data\n",
    "checkpoint_filepath = './modelcheckpoints' # path of checkpoint\n",
    "model_path = './model' #path of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9E63kZj5OAaL"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7704,
     "status": "ok",
     "timestamp": 1608395736647,
     "user": {
      "displayName": "Yingyu Cao",
      "photoUrl": "https://lh4.googleusercontent.com/-1hQkdsgYrWk/AAAAAAAAAAI/AAAAAAAAAE4/6dhXqWayhPk/s64/photo.jpg",
      "userId": "04224777957403955484"
     },
     "user_tz": 300
    },
    "id": "JmQ9Y3gZOAaL",
    "outputId": "0d7e3771-f15e-42c8-edd2-fee602e7a705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 64, 64, 3)\n",
      "Validation data shape: (20000, 64, 64, 3)\n",
      "Found 10000 validated image filenames belonging to 200 classes.\n",
      "Testing data shape: (10000, 64, 64, 3)\n",
      "End loading!\n"
     ]
    }
   ],
   "source": [
    "train_generator, val_generator, test_generator = load_data(data_path, img_width=64, img_height=64, \n",
    "                                           batch_size=128, augmentation=None, seed=None, load_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k9jo_65OAaM"
   },
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11627,
     "status": "ok",
     "timestamp": 1608395741617,
     "user": {
      "displayName": "Yingyu Cao",
      "photoUrl": "https://lh4.googleusercontent.com/-1hQkdsgYrWk/AAAAAAAAAAI/AAAAAAAAAE4/6dhXqWayhPk/s64/photo.jpg",
      "userId": "04224777957403955484"
     },
     "user_tz": 300
    },
    "id": "LwJBXiREOAaM",
    "outputId": "767cfee3-6802-460a-fa2f-212d7626fc3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 64, 64, 3)\n",
      "Validation data shape: (20000, 64, 64, 3)\n",
      "End loading!\n"
     ]
    }
   ],
   "source": [
    "data_aug_args = dict(CoarseDropout_range=(0.0, 0.05),\n",
    "                     CoarseDropout_size_percent=(0.02, 0.25),\n",
    "                     Affine_translate_percent=(-0.2, 0.2),\n",
    "                     Affine_scale=(0.5, 1.5),\n",
    "                     Affine_shear=(-20, 20),\n",
    "                     Affine_rotate=(-45, 45),\n",
    "                     Flip_percent=0.5,\n",
    "                     GaussianBlur_sigma=(0.0, 3.0),\n",
    "                     CropAndPad_percent=(-0.25, 0.25),\n",
    "                     Multiply=(0.5, 1.5),\n",
    "                     LinearContrast=(0.4, 1.6),\n",
    "                     AdditiveGaussianNoise_scale=0.2*255)\n",
    "\n",
    "aug = data_augmentation(complicated=False, **data_aug_args)\n",
    "\n",
    "\n",
    "train_generator_aug, val_generator_aug = load_data(data_path, img_width=64, img_height=64, \n",
    "                                                   batch_size=128, augmentation=aug, \n",
    "                                                   seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJa_Xrr9OAaN"
   },
   "source": [
    "## Model 1 Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4059,
     "status": "ok",
     "timestamp": 1608395742822,
     "user": {
      "displayName": "Yingyu Cao",
      "photoUrl": "https://lh4.googleusercontent.com/-1hQkdsgYrWk/AAAAAAAAAAI/AAAAAAAAAE4/6dhXqWayhPk/s64/photo.jpg",
      "userId": "04224777957403955484"
     },
     "user_tz": 300
    },
    "id": "zn7p8on3OAaO",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f7da6d6d-2d10-4320-cd9a-50ab947732e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 3 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, None, 3 128         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, None, 3 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 6 18432       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, None, 6 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 1 73728       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, None, 1 512         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, None, None, 1 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 2 294912      activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, None, 2 1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, None, None, 2 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 5 1179648     activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, None, 5 2048        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, None, None, 5 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, None, None, 5 0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 6 294912      max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, None, 6 256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, None, None, 6 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 73728       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, None, 1 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, None, None, 1 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 2 294912      activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, None, 2 1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, None, None, 2 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 5 1179648     activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, None, 5 2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, None, None, 5 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 4718592     activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, None, 1 4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, None, None, 1 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_SpaceToDepth (Tenso [(None, None, None,  0           max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, None, 1 0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 3 0           tf_op_layer_SpaceToDepth[0][0]   \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 3 884736      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, None, 3 128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, None, None, 3 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 1 36864       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, None, 1 512         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, None, None, 1 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 2 294912      activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, None, 2 1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, None, None, 2 0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 5 1179648     activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, None, 5 2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, None, None, 5 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 1 4718592     activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, None, 1 4096        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, None, None, 1 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_SpaceToDepth_1 (Ten [(None, None, None,  0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, None, 1 0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 1 0           tf_op_layer_SpaceToDepth_1[0][0] \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 2 2662400     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, None, 2 800         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 200)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 200)          0           global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 17,927,040\n",
      "Trainable params: 17,916,784\n",
      "Non-trainable params: 10,256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = init_model_1((None,None,3))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "iPu4s0VpOAaO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "# optimizer: Adam\n",
    "opt = Adam()\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aOKduQ6PS8m"
   },
   "source": [
    "### Train model 1 with 32x32 data and 8 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1480476,
     "status": "ok",
     "timestamp": 1608397239078,
     "user": {
      "displayName": "Yingyu Cao",
      "photoUrl": "https://lh4.googleusercontent.com/-1hQkdsgYrWk/AAAAAAAAAAI/AAAAAAAAAE4/6dhXqWayhPk/s64/photo.jpg",
      "userId": "04224777957403955484"
     },
     "user_tz": 300
    },
    "id": "BMicHBI_S3uE",
    "outputId": "84bfa8e9-d71b-4c97-9ba7-b9b202a64e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 32, 32, 3)\n",
      "Validation data shape: (20000, 32, 32, 3)\n",
      "End loading!\n",
      "Epoch 1/8\n",
      "200/200 [==============================] - 173s 864ms/step - loss: 4.7436 - accuracy: 0.0637 - val_loss: 5.6997 - val_accuracy: 0.0097 - lr: 0.0010\n",
      "Epoch 2/8\n",
      "200/200 [==============================] - 171s 855ms/step - loss: 4.3448 - accuracy: 0.1113 - val_loss: 5.2052 - val_accuracy: 0.0286 - lr: 0.0010\n",
      "Epoch 3/8\n",
      "200/200 [==============================] - 171s 855ms/step - loss: 4.1007 - accuracy: 0.1464 - val_loss: 4.4308 - val_accuracy: 0.0898 - lr: 0.0010\n",
      "Epoch 4/8\n",
      "200/200 [==============================] - 171s 856ms/step - loss: 3.9054 - accuracy: 0.1732 - val_loss: 4.1942 - val_accuracy: 0.1256 - lr: 0.0010\n",
      "Epoch 5/8\n",
      "200/200 [==============================] - 171s 855ms/step - loss: 3.7320 - accuracy: 0.2036 - val_loss: 3.9762 - val_accuracy: 0.1548 - lr: 0.0010\n",
      "Epoch 6/8\n",
      "200/200 [==============================] - 171s 853ms/step - loss: 3.5810 - accuracy: 0.2261 - val_loss: 4.1371 - val_accuracy: 0.1370 - lr: 0.0010\n",
      "Epoch 7/8\n",
      "200/200 [==============================] - 171s 853ms/step - loss: 3.4509 - accuracy: 0.2482 - val_loss: 4.0484 - val_accuracy: 0.1461 - lr: 0.0010\n",
      "Epoch 8/8\n",
      "200/200 [==============================] - 171s 856ms/step - loss: 3.3319 - accuracy: 0.2669 - val_loss: 3.7913 - val_accuracy: 0.1834 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# get data generator\n",
    "train_generator_32, val_generator_32 = load_data(data_path, img_width=32, img_height=32, \n",
    "                                           batch_size=128, augmentation=None, seed=None)\n",
    "# train model\n",
    "callback = ReduceLROnPlateau(patience=5, min_lr=6e-7)\n",
    "model1_1, history1_1 = training(model1, 'model1_1', train_generator_32, val_generator_32, \n",
    "                           callback, checkpoint_filepath, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1407,
     "status": "ok",
     "timestamp": 1608397402374,
     "user": {
      "displayName": "Yingyu Cao",
      "photoUrl": "https://lh4.googleusercontent.com/-1hQkdsgYrWk/AAAAAAAAAAI/AAAAAAAAAE4/6dhXqWayhPk/s64/photo.jpg",
      "userId": "04224777957403955484"
     },
     "user_tz": 300
    },
    "id": "lPcKgOW1lBvP",
    "outputId": "1f2115b5-fa6d-4267-e6a3-2ecbdca6b13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1_1_12-19_21-03.h5 saved at ./model!\n"
     ]
    }
   ],
   "source": [
    "# save model periodically\n",
    "save_model(model1_1, 'model1_1', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgFIYIf7Pv1H"
   },
   "source": [
    "### Train model 1 with 64x64 data and 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWWYpZiBPQ1X",
    "outputId": "06cbe299-b7a5-4526-a3ec-c07b714ad615"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 64, 64, 3)\n",
      "Validation data shape: (20000, 64, 64, 3)\n",
      "End loading!\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 657s 3s/step - loss: 3.4248 - accuracy: 0.2752 - val_loss: 3.8398 - val_accuracy: 0.1708 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 3.2477 - accuracy: 0.2971 - val_loss: 3.4940 - val_accuracy: 0.2228 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 3.1201 - accuracy: 0.3204 - val_loss: 3.5342 - val_accuracy: 0.2211 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 3.0076 - accuracy: 0.3362 - val_loss: 3.4691 - val_accuracy: 0.2409 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.9203 - accuracy: 0.3473 - val_loss: 3.4291 - val_accuracy: 0.2390 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.8368 - accuracy: 0.3595 - val_loss: 3.5410 - val_accuracy: 0.2229 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.7332 - accuracy: 0.3810 - val_loss: 3.0715 - val_accuracy: 0.3062 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.6308 - accuracy: 0.3993 - val_loss: 3.1793 - val_accuracy: 0.2830 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.5248 - accuracy: 0.4248 - val_loss: 3.1515 - val_accuracy: 0.2891 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 2.4728 - accuracy: 0.4301 - val_loss: 3.0599 - val_accuracy: 0.3006 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.3980 - accuracy: 0.4468 - val_loss: 3.0493 - val_accuracy: 0.3095 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.3094 - accuracy: 0.4645 - val_loss: 2.8237 - val_accuracy: 0.3559 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 654s 3s/step - loss: 2.2559 - accuracy: 0.4743 - val_loss: 2.8002 - val_accuracy: 0.3556 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.1622 - accuracy: 0.4962 - val_loss: 2.8134 - val_accuracy: 0.3542 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.1077 - accuracy: 0.5064 - val_loss: 2.7081 - val_accuracy: 0.3692 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# get data generator\n",
    "train_generator_64, val_generator_64 = load_data(data_path, img_width=64, img_height=64, \n",
    "                                           batch_size=128, augmentation=None, seed=None)\n",
    "# train model\n",
    "model1_2, history1_2 = training(model1_1, 'model1_2', train_generator_64, val_generator_64, \n",
    "                           callback, checkpoint_filepath, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYO4IYsrlVkf",
    "outputId": "4e2b815a-b1ec-4026-cca8-1b223ba6450c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1_2_12-19_23-47.h5 saved at ./model!\n"
     ]
    }
   ],
   "source": [
    "# save model periodically\n",
    "save_model(model1_2, 'model1_2', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNNJGlI8RMEX"
   },
   "source": [
    "### Train model 1 with 16x16 data and 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxLOyLFXRMEX",
    "outputId": "2c8ba9fc-57f1-47e9-f3ad-a30291108659"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 16, 16, 3)\n",
      "Validation data shape: (20000, 16, 16, 3)\n",
      "End loading!\n",
      "Epoch 1/5\n",
      "200/200 [==============================] - 56s 280ms/step - loss: 3.6970 - accuracy: 0.1988 - val_loss: 3.6112 - val_accuracy: 0.2152 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 3.1337 - accuracy: 0.2840 - val_loss: 3.5588 - val_accuracy: 0.2263 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 2.9498 - accuracy: 0.3224 - val_loss: 3.4739 - val_accuracy: 0.2382 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 2.7399 - accuracy: 0.3602 - val_loss: 3.4253 - val_accuracy: 0.2551 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 55s 277ms/step - loss: 2.5912 - accuracy: 0.3966 - val_loss: 3.3134 - val_accuracy: 0.2623 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# get data generator\n",
    "train_generator_16, val_generator_16 = load_data(data_path, img_width=16, img_height=16, \n",
    "                                           batch_size=128, augmentation=None, seed=None)\n",
    "# train model\n",
    "model1_3, history1_3 = training(model1_2, 'model1_3', train_generator_16, val_generator_16, \n",
    "                           callback, checkpoint_filepath, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87F9zQUsRMEX",
    "outputId": "78eee456-d591-44ef-9e06-132aa46f0da9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1_3_12-19_23-57.h5 saved at ./model!\n"
     ]
    }
   ],
   "source": [
    "# save model periodically\n",
    "save_model(model1_3, 'model1_3', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjusRDyIRMEX"
   },
   "source": [
    "### Train model 1 with 64x64 data and 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EC_7fhjRMEY",
    "outputId": "14e20cf1-7675-4668-af80-034d0296a906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin loading...\n",
      "Found 80000 images belonging to 200 classes.\n",
      "Found 20000 images belonging to 200 classes.\n",
      "Training data shape: (80000, 64, 64, 3)\n",
      "Validation data shape: (20000, 64, 64, 3)\n",
      "End loading!\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.4367 - accuracy: 0.4443 - val_loss: 2.7572 - val_accuracy: 0.3751 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 2.0930 - accuracy: 0.5083 - val_loss: 2.7404 - val_accuracy: 0.3756 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.9781 - accuracy: 0.5350 - val_loss: 2.6057 - val_accuracy: 0.3925 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 1.8943 - accuracy: 0.5509 - val_loss: 2.4974 - val_accuracy: 0.4119 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.8356 - accuracy: 0.5660 - val_loss: 2.4524 - val_accuracy: 0.4211 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 1.7760 - accuracy: 0.5759 - val_loss: 2.4042 - val_accuracy: 0.4287 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 651s 3s/step - loss: 1.7047 - accuracy: 0.5957 - val_loss: 2.5369 - val_accuracy: 0.4085 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.6281 - accuracy: 0.6134 - val_loss: 2.4667 - val_accuracy: 0.4221 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.5934 - accuracy: 0.6201 - val_loss: 2.3771 - val_accuracy: 0.4446 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 653s 3s/step - loss: 1.5112 - accuracy: 0.6408 - val_loss: 2.7352 - val_accuracy: 0.3774 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.4771 - accuracy: 0.6471 - val_loss: 2.5557 - val_accuracy: 0.4100 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.4001 - accuracy: 0.6668 - val_loss: 2.5749 - val_accuracy: 0.4101 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.3520 - accuracy: 0.6800 - val_loss: 2.4821 - val_accuracy: 0.4248 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 652s 3s/step - loss: 1.2862 - accuracy: 0.6966 - val_loss: 2.2778 - val_accuracy: 0.4654 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 651s 3s/step - loss: 1.2312 - accuracy: 0.7076 - val_loss: 2.3517 - val_accuracy: 0.4497 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# get data generator\n",
    "train_generator_64, val_generator_64 = load_data(data_path, img_width=64, img_height=64, \n",
    "                                           batch_size=128, augmentation=None, seed=None)\n",
    "# train model\n",
    "model1_4, history1_4 = training(model1_3, 'model1_4', train_generator_64, val_generator_64, \n",
    "                           callback, checkpoint_filepath, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCuSSi2dRMEY",
    "outputId": "1eae3171-ef6d-45ed-e22a-86392715f11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1_4_12-20_02-41.h5 saved at ./model!\n"
     ]
    }
   ],
   "source": [
    "# save model periodically\n",
    "save_model(model1_4, 'model1_4', model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main_model1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
